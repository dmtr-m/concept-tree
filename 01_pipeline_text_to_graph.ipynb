{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"[ [ [\", \"[[[\").replace(\"] ] ]\", \"]]]\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntactic_relations(doc, lemmatizer):\n",
    "    \"\"\"\n",
    "    Извлекает синтаксические связи (именные группы, включая прилагательные и артикли) из текста.\n",
    "    Обрабатывает сочетания глагола с предлогом как единое ребро.\n",
    "    Обрабатывает сочинение (conj), создавая дополнительные связи.\n",
    "    \"\"\"\n",
    "    chunks = []  # Список для хранения именных групп\n",
    "    relations = []  # Список для хранения связей (субъект, глагол, объект)\n",
    "    subjects = {}  # Словарь для хранения подлежащих\n",
    "    conjunctions = {}  # Словарь для хранения связей conj (сочинение)\n",
    "    chunk_to_text = {}  # Связываем root токены с текстом именных групп\n",
    "\n",
    "    # Добавляем именные группы (NOUN CHUNKS) и нормализуем их\n",
    "    for chunk in doc.noun_chunks:\n",
    "        normalized_chunk = ' '.join([lemmatizer.lemmatize(token.text.lower(), pos='n') for token in chunk if token.text.lower() not in ['the', 'a', 'an']])\n",
    "        chunks.append((chunk.start_char, chunk.end_char, chunk, normalized_chunk, chunk.root.head, chunk.root.dep_))\n",
    "        chunk_to_text[chunk.root] = normalized_chunk  # Связываем root токен с нормализованным текстом\n",
    "\n",
    "    # Обрабатываем сочинение (conj)\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"conj\" and token.head in chunk_to_text:\n",
    "            head_text = chunk_to_text[token.head]\n",
    "            conj_text = chunk_to_text.get(token, None)  # Используем уже обработанный chunk\n",
    "\n",
    "            if head_text and conj_text:\n",
    "                conjunctions.setdefault(head_text, []).append(conj_text)\n",
    "\n",
    "    # Добавляем подлежащие\n",
    "    for chunk in chunks:\n",
    "        if chunk[5] == 'nsubj':\n",
    "            subject_text = chunk_to_text.get(chunk[2].root, chunk[3])  # Используем нормализованный текст\n",
    "            subjects.setdefault(chunk[4], []).append(subject_text)\n",
    "\n",
    "            # Добавляем conj-подлежащие\n",
    "            if subject_text in conjunctions:\n",
    "                subjects[chunk[4]].extend(conjunctions[subject_text])\n",
    "\n",
    "    # Добавляем связи для глаголов и предлогов\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Связи с глаголами\n",
    "        if chunk[4].pos_ == 'VERB' and chunk[5] != 'nsubj':\n",
    "            subject_list = subjects.get(chunk[4], [])\n",
    "            object_text = chunk_to_text.get(chunk[2].root, chunk[3])  # Используем нормализованный текст\n",
    "\n",
    "            for subject in subject_list:\n",
    "                relations.append((subject, chunk[4].text, object_text))\n",
    "\n",
    "                # Добавляем conj-объекты\n",
    "                if object_text in conjunctions:\n",
    "                    for conj in conjunctions[object_text]:\n",
    "                        relations.append((subject, chunk[4].text, conj))\n",
    "\n",
    "        # Обрабатываем сочетания глаголов и предлогов как единое ребро\n",
    "        if chunk[4].pos_ == 'VERB' and i + 1 < len(chunks):\n",
    "            next_chunk = chunks[i + 1]\n",
    "            if next_chunk[4].pos_ == 'ADP':  # Если следующий элемент - предлог\n",
    "                subject_list = subjects.get(chunk[4], [])\n",
    "                relation_text = f\"{chunk[4].text} {next_chunk[4].text}\"\n",
    "                object_text = chunk_to_text.get(next_chunk[2].root, next_chunk[3])\n",
    "\n",
    "                for subject in subject_list:\n",
    "                    relations.append((subject, relation_text, object_text))\n",
    "\n",
    "                    # Добавляем conj-объекты\n",
    "                    if object_text in conjunctions:\n",
    "                        for conj in conjunctions[object_text]:\n",
    "                            relations.append((subject, relation_text, conj))\n",
    "\n",
    "    # Добавляем связи для предлогов и объектов\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"prep\" and token.head.pos_ == \"NOUN\":\n",
    "            prep_text = token.text\n",
    "            object_text = None\n",
    "\n",
    "            # Ищем объект предлога (pobj)\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"pobj\":\n",
    "                    object_text = chunk_to_text.get(child, child.text.lower())\n",
    "\n",
    "            if object_text:\n",
    "                head_text = chunk_to_text.get(token.head, token.head.text.lower())  # Нормализуем ключ\n",
    "                subject_list = [head_text]\n",
    "\n",
    "                # Добавляем все сочинённые существительные\n",
    "                if head_text in conjunctions:\n",
    "                    subject_list.extend(conjunctions[head_text])\n",
    "\n",
    "                # Добавляем рёбра для каждого существительного в subject_list\n",
    "                for subject in subject_list:\n",
    "                    relations.append((subject, prep_text, object_text))\n",
    "                    # print(f\"Adding edge: {subject} --[{prep_text}]--> {object_text}\")\n",
    "\n",
    "    # Удаляем связи, где субъект \"that\"\n",
    "    relations = [rel for rel in relations if rel[0] != \"that\"]\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdemyokhin_1/.conda/envs/venv_01_pipeline/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_triplets(text: str) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    triplets = get_syntactic_relations(doc, lemmatizer)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_word(word: str) -> str:\n",
    "    reference = False\n",
    "    if word.startswith(\"[ [ [ \"):\n",
    "        reference = True\n",
    "    word = word.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    word = word.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    word = word.replace(\",\", \"\").replace(\";\", \"\")\n",
    "    word = word.replace(\"-\", \"\").replace(\".\", \"\")\n",
    "    word = word.replace(\":\", \"\").replace(\".\", \"\")\n",
    "    word = word.replace(\"‘\", \"\").replace(\"’\", \"\")\n",
    "    word = word.strip()\n",
    "    word = re.sub(r'\\s{2,}', ' ', word).strip()\n",
    "    if reference:\n",
    "        word = \"[[[\" + word + \"]]]\"\n",
    "    return word\n",
    "\n",
    "def clean_triplet(triplet: tuple[str]) -> tuple[str]:\n",
    "    triplet = (clean_word(word) for word in triplet)\n",
    "    return triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def process_file(input_file: str, output_file: str):\n",
    "    with open(input_file, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text = clean_text(text)\n",
    "    triplets = get_triplets(text)\n",
    "\n",
    "    writer = csv.writer(open(output_file, 'w+', encoding=\"utf-8\"), delimiter=\";\")\n",
    "    for triplet in triplets:\n",
    "        triplet = clean_triplet(triplet)\n",
    "        writer.writerow(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "input_directory = Path(\"/home/kdemyokhin_1/concept-tree-course-work/articles_anaphora_resolved/arxiv-txt-cs\")\n",
    "output_directory = Path(\"/home/kdemyokhin_1/concept-tree-course-work/articles_triples/arxiv-txt-cs\")\n",
    "\n",
    "\n",
    "# Получаем список всех txt файлов рекурсивно (включая поддиректории)\n",
    "input_files = list(input_directory.rglob(\"*.txt\"))\n",
    "\n",
    "# Формируем список выходных файлов, сохраняя структуру поддиректорий\n",
    "output_files = []\n",
    "for file in input_files:\n",
    "    # Вычисляем относительный путь файла относительно input_directory\n",
    "    relative_path = file.relative_to(input_directory)\n",
    "    # Формируем путь к файлу в выходной директории\n",
    "    out_file = output_directory / relative_path\n",
    "    # Создаем директорию, если её ещё нет\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_files.append(out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 122/10959 [00:43<2:14:03,  1.35it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for input_path, output_path in tqdm.tqdm(zip(input_files, output_files), total=len(input_files)):\n",
    "    if os.path.exists(output_path):\n",
    "        continue\n",
    "    try:\n",
    "        process_file(input_path, output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Can't process {input_path}: {e}\")\n",
    "        with open(f\"/home/kdemyokhin_1/concept-tree-course-work/articles_triples/unprocessed_files.txt\",\"w+\") as f:\n",
    "            f.write(str(input_path)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
