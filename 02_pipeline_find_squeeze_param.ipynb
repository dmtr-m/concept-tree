{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitry/Desktop/Projects/concept-tree/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from graph.graph import Graph\n",
    "import tqdm\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "text_limit = 3\n",
    "\n",
    "def extract_txt_files(root_dir):\n",
    "    txt_files = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        files_in_dir = 0\n",
    "        for file in filenames:\n",
    "            if file.endswith('.txt'):\n",
    "                full_path = os.path.join(dirpath, file)\n",
    "                txt_files.append(full_path)\n",
    "                files_in_dir += 1\n",
    "                if files_in_dir > text_limit:\n",
    "                    break\n",
    "\n",
    "    return txt_files\n",
    "\n",
    "# Example usage\n",
    "root_directory = 'process_text/data/arxiv-txt-cs/'  # Replace with your root directory path\n",
    "txt_file_paths = extract_txt_files(root_directory)\n",
    "\n",
    "print(len(txt_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:20<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "added_edges = set()\n",
    "\n",
    "import csv\n",
    "\n",
    "for path in tqdm.tqdm(txt_file_paths):\n",
    "    with open(path, newline='') as csvfile:\n",
    "        triplets_reader = csv.reader(csvfile, delimiter=\";\")\n",
    "        for triplet in triplets_reader:\n",
    "            agent_1, action, agent_2 = triplet\n",
    "            edge = (\n",
    "                agent_1,\n",
    "                agent_2,\n",
    "                action,\n",
    "            )\n",
    "            if len(agent_1) == 0 or len(agent_2) == 0 or len(action) == 0:\n",
    "                continue\n",
    "\n",
    "            # if edge[0] == \"+30\" or edge[1] == \"+30\":\n",
    "            #     continue\n",
    "\n",
    "            if \"id\" in edge[0] or \"id\" in edge[1] or \"id\" in edge[2]:\n",
    "                continue\n",
    "\n",
    "            if \"im\" in edge[0] or \"im\" in edge[1] or \"im\" in edge[2]:\n",
    "                continue\n",
    "            \n",
    "            if \"3mb\" in edge[0] or \"3mb\" in edge[1] or \"3mb\" in edge[2]:\n",
    "                continue\n",
    "\n",
    "            if \"10mb\" in edge[0] or \"10mb\" in edge[1] or \"10mb\" in edge[2]:\n",
    "                continue\n",
    "\n",
    "            if \"nsga\" in edge[0] or \"nsga\" in edge[1] or \"nsga\" in edge[2]:\n",
    "                continue\n",
    "                \n",
    "            if \"%\" in edge[0] or \"%\" in edge[1] or \"%\" in edge[2]:\n",
    "                continue\n",
    "\n",
    "            if edge[0].count(\" \") > 2 or edge[1].count(\" \") > 2 or edge[2].count(\" \") > 2:\n",
    "                continue\n",
    "\n",
    "            if len(edge[0]) < 4 or len(edge[1]) < 4:\n",
    "                continue\n",
    "\n",
    "            if edge not in added_edges:\n",
    "                added_edges.add(edge)\n",
    "                if not graph.contains_vertex(edge[0]):\n",
    "                    graph.add_vertex(edge[0])\n",
    "                if not graph.contains_vertex(edge[1]):\n",
    "                    graph.add_vertex(edge[1])\n",
    "                graph.add_edge(*edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'words_of_concept'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCluster vertices\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprocess_graph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msqueezing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m squeeze\n\u001b[1;32m      7\u001b[0m (\n\u001b[1;32m      8\u001b[0m     (words_to_cluster, merged_words_map),\n\u001b[1;32m      9\u001b[0m     (bigrams_to_cluster, merged_bigrams_map),\n\u001b[1;32m     10\u001b[0m     (trigrams_to_cluster, merged_trigrams_map),\n\u001b[0;32m---> 11\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.46\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.46\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.41\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.46\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.71\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_vertices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/concept-tree/process_graph/squeezing.py:20\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(eps, vertices)\u001b[0m\n\u001b[1;32m     17\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     18\u001b[0m trigrams \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, data \u001b[38;5;129;01min\u001b[39;00m vertices:\n\u001b[1;32m     21\u001b[0m     words_of_concept \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords_of_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m     22\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'words_of_concept'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cluster vertices\n",
    "\"\"\"\n",
    "\n",
    "from process_graph.squeezing import squeeze\n",
    "\n",
    "(\n",
    "    (words_to_cluster, merged_words_map),\n",
    "    (bigrams_to_cluster, merged_bigrams_map),\n",
    "    (trigrams_to_cluster, merged_trigrams_map),\n",
    ") = squeeze((0.46, 0.46 * 1.41, 0.46 * 1.71), graph.get_all_vertices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon = []\n",
    "max_cluster_size = []\n",
    "average_cluster_size = []\n",
    "median_cluster_size = []\n",
    "\n",
    "def find_squeeze_params():    \n",
    "    for eps in np.arange(0.2, 0.5, 0.02):\n",
    "        (\n",
    "            (words_to_cluster, merged_words_map),\n",
    "            (bigrams_to_cluster, merged_bigrams_map),\n",
    "            (trigrams_to_cluster, merged_trigrams_map),\n",
    "        ) = squeeze((eps, eps * 1.41, eps * 1.71), graph.vertices.values())\n",
    "\n",
    "        cluster_sizes = []\n",
    "        for cluster in merged_words_map.values():\n",
    "            if len(cluster) > 1:\n",
    "                cluster_sizes.append(len(cluster))\n",
    "\n",
    "        if len(cluster_sizes) < 1:\n",
    "            continue\n",
    "        epsilon.append(eps)\n",
    "        max_cluster_size.append(np.max(cluster_sizes))\n",
    "        average_cluster_size.append(np.mean(cluster_sizes))\n",
    "        median_cluster_size.append(np.median(cluster_sizes))\n",
    "\n",
    "find_squeeze_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 2.5), layout='constrained')\n",
    "\n",
    "axs[0].plot(epsilon, max_cluster_size)\n",
    "axs[0].set_title(\"max cluster size\")\n",
    "axs[0].set_ylim((0, 25))\n",
    "\n",
    "axs[1].plot(epsilon, average_cluster_size)\n",
    "axs[1].set_title(\"average cluster size\")\n",
    "axs[1].set_ylim((0, 25));\n",
    "\n",
    "# axs[2].plot(epsilon, median_cluster_size)\n",
    "# axs[2].set_title(\"mean cluster size\")\n",
    "# axs[2].set_ylim((0, 20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Words:\")\n",
    "# for item in merged_words_map.items():\n",
    "#     if len(item[1]) > 1:\n",
    "#         print(item)\n",
    "\n",
    "print(\"Bigrams:\")\n",
    "for item in merged_bigrams_map.items():\n",
    "    if len(item[1]) > 1:\n",
    "        print(item)\n",
    "\n",
    "# print(\"Trigrams:\")\n",
    "# for item in merged_trigrams_map.items():\n",
    "#     if len(item[1]) > 1:\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster edges\n",
    "\"\"\"\n",
    "from process_graph.edges_clustering import cluster_and_evaluate_all_sizes\n",
    "\n",
    "labels = []\n",
    "embeddings = []\n",
    "\n",
    "for edge in graph.edges:\n",
    "    labels.append(edge.label)\n",
    "    embeddings.append(edge.embedding)\n",
    "\n",
    "edge_maps = cluster_and_evaluate_all_sizes(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    {\n",
    "        100: {\n",
    "            \"model\": \"DBSCAN\",\n",
    "            \"params\": {\n",
    "                \"eps\": 0.5,\n",
    "                \"min_samples\": 2\n",
    "            }\n",
    "        },\n",
    "        200: {\n",
    "            \"model\": \"DBSCAN\",\n",
    "            \"params\": {\n",
    "                \"eps\": 0.5,\n",
    "                \"min_samples\": 2\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, clusters = edge_maps\n",
    "\n",
    "edge_map_word = clusters[100]\n",
    "edge_map_bigram = clusters[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "clustered_graph = Graph()\n",
    "\n",
    "for new_vertice_words in merged_words_map.keys():\n",
    "    clustered_graph.add_vertex(new_vertice_words)\n",
    "for new_vertice_bigram in merged_bigrams_map.keys():\n",
    "    clustered_graph.add_vertex(new_vertice_bigram)\n",
    "for new_vertice_trigram in merged_trigrams_map.keys():\n",
    "    clustered_graph.add_vertex(new_vertice_trigram)\n",
    "\n",
    "def match_new_vertice(label: str) -> str:\n",
    "    if label in words_to_cluster:\n",
    "        return words_to_cluster[label]\n",
    "    elif label in bigrams_to_cluster:\n",
    "        return bigrams_to_cluster[label]\n",
    "    elif label in trigrams_to_cluster:\n",
    "        return trigrams_to_cluster[label]\n",
    "    return label\n",
    "\n",
    "def match_new_edge(label: str) -> str:\n",
    "    if label in edge_map_word:\n",
    "        return edge_map_word[label]\n",
    "    if label in edge_map_bigram:\n",
    "        return edge_map_bigram[label]\n",
    "    return label\n",
    "\n",
    "added_edges = set()  # keeps added_edges in (agent_1, agent_2, label) format\n",
    "\n",
    "for edge in tqdm.tqdm(graph.edges):\n",
    "\n",
    "    new_edge = (\n",
    "        match_new_vertice(edge.agent_1),\n",
    "        match_new_vertice(edge.agent_2),\n",
    "        match_new_edge(edge.label)\n",
    "    )\n",
    "    if new_edge in added_edges:\n",
    "        continue\n",
    "    added_edges.add(new_edge)\n",
    "\n",
    "    if new_edge[0] not in clustered_graph.vertices:\n",
    "        clustered_graph.add_vertex(new_edge[0])\n",
    "    if new_edge[1] not in clustered_graph.vertices:\n",
    "        clustered_graph.add_vertex(new_edge[1])\n",
    "\n",
    "    clustered_graph.add_edge(*new_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from directed_graph.visualize_graph import visualize_graph_ngrams\n",
    "\n",
    "visualize_graph_ngrams(clustered_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(graph))\n",
    "print(repr(clustered_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics['embedding_size_100']['Cluster Sizes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_count = 0\n",
    "\n",
    "for v in metrics['embedding_size_200']['Cluster Sizes'].values():\n",
    "    if v > 20:\n",
    "        v_count += 1\n",
    "\n",
    "print(v_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustered_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "edge_cluster_to_word_map = defaultdict(list)\n",
    "for key, value in edge_map_word.items():\n",
    "    edge_cluster_to_word_map[value].append(key)\n",
    "for key, value in edge_map_word.items():\n",
    "    edge_cluster_to_word_map[value].append(key)\n",
    "edge_cluster_to_word_map = dict(edge_cluster_to_word_map)\n",
    "\n",
    "edge_cluster_to_bigram_map = defaultdict(list)\n",
    "for key, value in edge_map_bigram.items():\n",
    "    edge_cluster_to_bigram_map[value].append(key)\n",
    "for key, value in edge_map_bigram.items():\n",
    "    edge_cluster_to_bigram_map[value].append(key)\n",
    "edge_cluster_to_bigram_map = dict(edge_cluster_to_bigram_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_beautiful_dict(d):\n",
    "    \"\"\"Prints a dictionary with pretty formatting.\"\"\"\n",
    "    print(json.dumps(d, indent=4, sort_keys=True))\n",
    "\n",
    "print_beautiful_dict(edge_cluster_to_word_map)\n",
    "# print_beautiful_dict(edge_cluster_to_bigram_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in edge_cluster_to_bigram_map:\n",
    "    if len(edge_cluster_to_bigram_map[k]) > 2:\n",
    "        print(k, edge_cluster_to_bigram_map[k][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
