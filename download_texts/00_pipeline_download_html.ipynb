{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "BASE_URL = \"https://arxiv.org\"\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
    "]\n",
    "\n",
    "def get_headers():\n",
    "    return {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "\n",
    "def get_topics():\n",
    "    response = requests.get(BASE_URL + \"/\", headers=get_headers())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics = []\n",
    "    for a in soup.select(\"a[href^='/archive/']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if re.match(r\"/archive/[\\w\\-]+\", href):\n",
    "            topics.append(href.split(\"/\")[-1])\n",
    "    return list(set(topics))\n",
    "\n",
    "def get_years_for_topic(topic):\n",
    "    url = f\"{BASE_URL}/archive/{topic}\"\n",
    "    response = requests.get(url, headers=get_headers())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    years = []\n",
    "    for a in soup.select(f\"a[href^='/year/{topic}/']\"):\n",
    "        match = re.search(rf'/year/{topic}/(\\d{{4}})', a['href'])\n",
    "        if match:\n",
    "            years.append(match.group(1))\n",
    "    return sorted(set(years))\n",
    "\n",
    "def get_months_for_year(topic, year):\n",
    "    months = [f\"{month:02d}\" for month in range(1, 13)]\n",
    "    return months\n",
    "\n",
    "def get_html_links(topic, year, month):\n",
    "    url = f\"{BASE_URL}/list/{topic}/{year}-{month}?skip=0&show=2000\"\n",
    "    response = requests.get(url, headers=get_headers())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for dt in soup.find_all(\"dt\"):\n",
    "        html_link = dt.find(\"a\", href=re.compile(r\"^https://arxiv.org/html/\\d{4}\\.\\d{5}(v\\d+)?\"))\n",
    "        if html_link:\n",
    "            links.append(html_link[\"href\"])\n",
    "            print(\"Found HTML article:\", html_link[\"href\"])\n",
    "        time.sleep(0.1)  # Politeness delay between items\n",
    "    return links\n",
    "\n",
    "def scrape_arxiv():\n",
    "    topics = get_topics()\n",
    "    for topic in topics:\n",
    "        print(f\"Processing topic: {topic}\")\n",
    "        years = get_years_for_topic(topic)\n",
    "        for year in years:\n",
    "            all_html_links = []\n",
    "            for month in get_months_for_year(topic, year):\n",
    "                print(f\"  Checking {year}-{month}\")\n",
    "                links = get_html_links(topic, year, month)\n",
    "                all_html_links.extend(links)\n",
    "                time.sleep(random.uniform(2, 5))  # Delay between months\n",
    "            if all_html_links:\n",
    "                os.makedirs(f\"arxiv_links/{topic}\", exist_ok=True)\n",
    "                with open(f\"arxiv_links/{topic}/{year}.txt\", \"w\") as f:\n",
    "                    for link in all_html_links:\n",
    "                        f.write(link + \"\\n\")\n",
    "                print(f\"Saved {len(all_html_links)} links for {topic}/{year}\")\n",
    "            time.sleep(random.uniform(5, 10))  # Delay between years\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_arxiv()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
