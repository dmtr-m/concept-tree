{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = \"1709.00284v2.pdf\"\n",
    "input_path = \"A subgraph matching algorithm based on subgraph index for knowledge graph.pdf\"\n",
    "output_path = \"clean_text_output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_clean_text(pdf_path, output_txt_path):\n",
    "    # Open the PDF file\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    total_pages = len(pdf)\n",
    "    all_pages_lines = []\n",
    "\n",
    "    # First pass: extract text lines from each page\n",
    "    for page_index in range(total_pages):\n",
    "        page = pdf[page_index]\n",
    "        text = page.get_text(\"text\")  # Extract text preserving layout-based order\n",
    "        if text is None:\n",
    "            all_pages_lines.append([])  # no text on this page (e.g. blank or image-only)\n",
    "        else:\n",
    "            # Split into individual lines\n",
    "            lines = text.splitlines()\n",
    "            all_pages_lines.append(lines)\n",
    "    pdf.close()\n",
    "\n",
    "    # Detect repeating headers/footers by analyzing first and last lines of each page\n",
    "    header_counts = {}\n",
    "    footer_counts = {}\n",
    "    for i, lines in enumerate(all_pages_lines):\n",
    "        if not lines or len(lines) == 0:\n",
    "            continue\n",
    "        first_line = lines[0].strip()\n",
    "        last_line = lines[-1].strip()\n",
    "        if first_line:\n",
    "            header_counts[first_line] = header_counts.get(first_line, 0) + 1\n",
    "        if last_line:\n",
    "            footer_counts[last_line] = footer_counts.get(last_line, 0) + 1\n",
    "\n",
    "    # Determine which lines are frequent headers/footers (appearing on >= 50% of pages)\n",
    "    headers_to_remove = {line for line, count in header_counts.items() if count >= 0.5 * total_pages}\n",
    "    footers_to_remove = {line for line, count in footer_counts.items() if count >= 0.5 * total_pages}\n",
    "\n",
    "    # Now, clean and filter lines\n",
    "    cleaned_lines = []\n",
    "    in_references = False\n",
    "    for page_index, lines in enumerate(all_pages_lines):\n",
    "        for line in lines:\n",
    "            if line is None:\n",
    "                continue\n",
    "            raw = line  # original line\n",
    "            text = raw.strip()\n",
    "\n",
    "            # Stop if we reached References section\n",
    "            if not in_references:\n",
    "                low = text.lower()\n",
    "                if low.startswith(\"references\") or low.startswith(\"bibliography\"):\n",
    "                    in_references = True\n",
    "                    break  # stop processing this page (and subsequent pages)\n",
    "            else:\n",
    "                break\n",
    "            # Skip if this line is a detected header or footer\n",
    "            if text in headers_to_remove or text in footers_to_remove:\n",
    "                continue\n",
    "            # Remove pure page numbers (e.g. \"12\")\n",
    "            if re.fullmatch(r'\\d+', text):\n",
    "                continue\n",
    "            # Remove \"Table of Contents\" if present (skip all lines until an empty line or main content)\n",
    "            # (In our case, academic articles usually don't include TOC, so this can be omitted or implemented if needed.)\n",
    "\n",
    "            # Remove \"Keywords\" section lines entirely\n",
    "            if text.lower().startswith(\"keywords\"):\n",
    "                # skip the entire keywords section (often one or two lines) \n",
    "                # skip this line and continue to next until an empty line\n",
    "                continue  # skip the \"Keywords: ...\" line\n",
    "            # If currently in a Keywords section (could add logic to skip subsequent keyword lines until blank)\n",
    "\n",
    "            # Remove the \"Abstract\" label if it‚Äôs at the start of the line\n",
    "            if text.lower().startswith(\"abstract\"):\n",
    "                cleaned_lines = []\n",
    "                # If the line is just \"Abstract\" or \"Abstract:\" and nothing else, skip it\n",
    "                parts = text.split(None, 1)\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    # Remove the word \"Abstract\" and keep the rest of the line\n",
    "                    text = parts[1].lstrip(\":-‚Äì \")  # remove any colon or dash after 'Abstract'\n",
    "            # Remove section headings (Introduction, Conclusion, etc.)\n",
    "            # Check common headings or numbered headings\n",
    "            lower_txt = text.lower().rstrip(\":\")\n",
    "            common_headings = {\"abstract\", \"introduction\", \"background\", \"related work\", \n",
    "                               \"methods\", \"materials\", \"methodology\", \"experiments\", \n",
    "                               \"results\", \"discussion\", \"conclusion\", \"conclusions\", \n",
    "                               \"acknowledgments\", \"acknowledgements\"}\n",
    "            if lower_txt in common_headings:\n",
    "                continue\n",
    "            # Check for numbered section headings like \"1. Introduction\" or \"2.1 Methods\"\n",
    "            if re.match(r'^\\d+(\\.\\d+)*\\s+', text):\n",
    "                # Remove leading numbers and dots and re-check\n",
    "                without_num = re.sub(r'^\\d+(\\.\\d+)*\\s+', '', text)\n",
    "                if without_num.lower() in common_headings:\n",
    "                    continue\n",
    "\n",
    "            # Remove figure and table captions (lines starting with \"Figure\", \"Fig\", or \"Table\")\n",
    "            if re.match(r'^(figure|fig|table)\\b', text, flags=re.IGNORECASE):\n",
    "                # If the line clearly starts a caption, skip it\n",
    "                # (Assumes that normal sentences rarely start with \"Figure\" or \"Table\")\n",
    "                continue\n",
    "\n",
    "            # # Remove inline citation markers like ‚Äú[12]‚Äù or ‚Äú(1999)‚Äù from the line\n",
    "            text = re.sub(r'\\[[0-9,\\s]+\\]', '', text)         # remove [1], [2, 5], etc.\n",
    "            text = re.sub(r'\\([A-Za-z]+ et al\\.?, \\d{4}\\)', '', text)  # remove (Name et al., 2020) patterns (basic approach)\n",
    "            text = re.sub(r'\\([0-9]{4}\\)', '', text)          # remove (2020) year-only citations\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)  # collapse multiple spaces that might result\n",
    "\n",
    "            # Append the cleaned text line (preserve any original newline structure by keeping empty lines as separators)\n",
    "            if text == \"\":\n",
    "                cleaned_lines.append(\"\")  # keep an empty line if it was blank (paragraph break)\n",
    "            else:\n",
    "                cleaned_lines.append(text)\n",
    "\n",
    "        if in_references:\n",
    "            break  # stop processing further pages once references start\n",
    "\n",
    "    # Merge hyphenated words that were split across lines\n",
    "    merged_lines = []\n",
    "    for line in cleaned_lines:\n",
    "        if merged_lines and merged_lines[-1].endswith('-') and line and line[0].islower():\n",
    "            # Merge with previous line: remove trailing hyphen and concatenate\n",
    "            merged_lines[-1] = merged_lines[-1][:-1] + line.lstrip()\n",
    "        else:\n",
    "            merged_lines.append(line)\n",
    "\n",
    "    # Join lines into continuous text, preserving paragraph breaks\n",
    "    output_text = \"\"\n",
    "    prev_line_blank = False\n",
    "    for line in merged_lines:\n",
    "        if line == \"\":\n",
    "            # blank line indicates a paragraph break\n",
    "            if not prev_line_blank:  # avoid multiple blank lines in a row\n",
    "                output_text += \"\\n\"\n",
    "            prev_line_blank = True\n",
    "        else:\n",
    "            if output_text and not prev_line_blank:\n",
    "                output_text += \" \"  # add space before concatenating if within paragraph\n",
    "            output_text += \" \" + line.strip()\n",
    "            prev_line_blank = False\n",
    "    \n",
    "    output_text = output_text.replace(\"i.e.\", \"\").replace(\"e.g.\", \"\").replace(\"etc\", \"\").replace(\"Eq.\", \"\").replace(\"Refs.\", \"\")\n",
    "    sentences = output_text.replace(\". \", \".\\n\").split(\"\\n\")\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        # Remove lines that are likely formulas or non-text (by character analysis)\n",
    "        # If the line has very few letters compared to other chars, or contains specific math symbols, skip it.\n",
    "        letters = sum(ch.isalpha() for ch in sentence)\n",
    "        digits = sum(ch.isdigit() for ch in sentence)\n",
    "        # Count non-alphanumeric, non-space characters\n",
    "        others = sum((not ch.isalnum() and not ch.isspace()) for ch in sentence)\n",
    "        # If no letters at all, skip (e.g. an equation number or purely symbols)\n",
    "        if letters == 0:\n",
    "            continue\n",
    "        # If the line is short (e.g. under 5 chars) and not ending in punctuation, skip it (likely fragment)\n",
    "        if len(sentence) < 5 and sentence[-1] not in \".?!\":\n",
    "            continue\n",
    "        # If letters are less than half of the characters (indicates lots of symbols/digits), skip\n",
    "        if letters / (letters + digits + others) < 0.5:\n",
    "            continue\n",
    "        # If contains obvious math symbols or notation, skip\n",
    "        if re.search(r'[=<>¬±√ó√∑‚àë‚àö‚â§‚â•œÜœÅŒ®œÉùêø‚àà‚Üí‚â†~‚ü®‚ü©]', sentence):\n",
    "            continue\n",
    "\n",
    "        cleaned_sentences.append(sentence)\n",
    "\n",
    "    output_text = \"\\n\".join(cleaned_sentences)\n",
    "\n",
    "    # Final cleanup: remove any spaces before punctuation and multiple spaces\n",
    "    output_text = re.sub(r\"\\s+([.,;:!?])\", r\"\\1\", output_text)\n",
    "    output_text = re.sub(r\" {2,}\", \" \", output_text)\n",
    "\n",
    "    # output_text = re.sub(r'\\s*\\([^)]*\\)\\s*', ' ', output_text)\n",
    "    output_text = re.sub(r'\\s*\\[[^\\]]*\\]\\s*', ' ', output_text)\n",
    "    # output_text = re.sub(r'\\s{2,}', ' ', output_text).strip()\n",
    "\n",
    "    # Write the clean text to output file\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_f:\n",
    "        out_f.write(output_text)\n",
    "\n",
    "# Process the sample PDFs and save outputs\n",
    "extract_clean_text(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example ! And another .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is an example [abc123]! And another [xyz@#].\"\n",
    "cleaned = re.sub(r'\\s*\\[[^\\]]*\\]\\s*', ' ', text)\n",
    "cleaned = re.sub(r'\\s{2,}', ' ', cleaned).strip()\n",
    "\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "pdf_path = \"/content/1709.00284v2.pdf\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "rendered = converter(pdf_path)\n",
    "text, _, _ = text_from_rendered(rendered)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
