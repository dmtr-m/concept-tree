{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy --q\n",
        "!python -m spacy download ru_core_news_lg --q\n",
        "!pip install pymorphy3 --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP5j0yWoz9oB",
        "outputId": "4f2450c6-eaaa-43c1-94c4-17363afb115c",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BAM2FR6dzvv0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "\n",
        "import types\n",
        "from types import NoneType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Иван Иванов перестал работать по рекомендации Всемирной Организации Здравоохранения. По этой причине его уволили из Яндекса.'"
      ],
      "metadata": {
        "id": "6HdtRNkW0TTr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_lg\")\n",
        "doc = nlp(text)\n",
        "\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "AJQeTF8K5b19"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def head_in_named_entity(doc, span): # на вход подаются документ и именованная сущность, которая в нём содержится\n",
        "    span_parts = span.text.split(' ')\n",
        "    head = None\n",
        "    heads = [[], []] # [[token], [head]]\n",
        "    for span_part in span_parts:\n",
        "        for token in doc: # перебираем токены, потому что именно они, в отличие от строк, содержат всю информацию\n",
        "            if span_part == token.text:\n",
        "                heads[0].append(token)\n",
        "                heads[1].append(token.head)\n",
        "    for i in range(len(span_parts)):\n",
        "        if heads[1][i] not in heads[0]: # вершиной является то, что не зависит от других слов, входящих в именованную сущность\n",
        "            head = heads[0][i]\n",
        "    return head, [_.split('=')[1] for _ in str(head.morph).split('|')], head.head, head.dep_, heads"
      ],
      "metadata": {
        "id": "vQLSZI925ZMN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_noun_phrase(doc, np): # на вход подаётся документ и именная группа\n",
        "    head, morphology, parent, dep, np_parts = head_in_named_entity(doc, np)\n",
        "    ana = morph.parse(head.text)[0]\n",
        "    res = ''\n",
        "    for i, np_part in enumerate(np_parts[0]):\n",
        "        np_part_head = np_parts[1][i]\n",
        "        if np_part == head:\n",
        "            np_part = ana.normal_form\n",
        "        else:\n",
        "            np_part = morph.parse(np_part.text)[0]\n",
        "            pos = str(np_part.tag).split(',')[0].split(' ')[0]\n",
        "            if pos == 'ADJF' and np_part_head == head:\n",
        "                gender, number = str(ana.normalized.tag).split(',')[2].split()\n",
        "                np_part = np_part.inflect({gender, 'nomn'})[0]\n",
        "            else:\n",
        "                np_part = np_part.word\n",
        "        res += np_part + ' '\n",
        "    return res.strip()"
      ],
      "metadata": {
        "id": "CzuXR6dT-phW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция ниже заменяет местоимения на именные группы и выделяет именные группы вместе с глаголами, от которых они зависят."
      ],
      "metadata": {
        "id": "J41R8CrwBsGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_syntactic_relations(doc):\n",
        "    chunks = [] # тут содержатся как именованные сущности, так и просто существительные\n",
        "    res = {} # {(индекс первого символа, индекс последнего символа): чанк в тексте, нормализованный чанк, родитель чанка, тип зависимости}\n",
        "    for ent in doc.ents: # добавляем именованные сущности\n",
        "        chars = (ent.start_char, ent.end_char)\n",
        "        chunks.append((chars, ent, ) + head_in_named_entity(doc, ent)[1:-1])\n",
        "        res[chars] = (ent.text, normalize_noun_phrase(doc, ent), chunks[-1][3], chunks[-1][4])\n",
        "    for token in doc: # добавляем существительные\n",
        "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "            morph = [_.split('=')[1] for _ in str(token.morph).split('|')]\n",
        "            chars = (token.idx, token.idx + len(token.text))\n",
        "            chunks.append((chars, token, morph, token.head, token.dep_))\n",
        "            res[chars] = (token.text, str(token.lemma_), chunks[-1][3], chunks[-1][4])\n",
        "    chunks.sort(key=lambda x: x[0])\n",
        "    for token in doc: # решаем анафору\n",
        "        if token.pos_ == 'PRON':\n",
        "            morph = [_.split('=')[1] for _ in str(token.morph).split('|')]\n",
        "            for chunk in chunks:\n",
        "                if chunk[0][0] < token.idx and chunk[2][2:4] == morph[1:3]:\n",
        "                    chars = (token.idx, token.idx + len(token.text))\n",
        "                    res[chars] = (token.text, normalize_noun_phrase(doc, chunk[1]), chunk[3], chunk[4])\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "Tbuo4mN2CXkc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_syntactic_relations(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWW_EJU5Emz7",
        "outputId": "c3ffcdc7-2a4a-4c7b-a81c-988583195ff8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0, 11): ('Иван Иванов', 'иван иванов', перестал, 'nsubj'), (46, 83): ('Всемирной Организации Здравоохранения', 'всемирная организация здравоохранения', рекомендации, 'nmod'), (116, 123): ('Яндекса', 'яндекс', уволили, 'obl'), (0, 4): ('Иван', 'иван', перестал, 'nsubj'), (5, 11): ('Иванов', 'иванов', Иван, 'appos'), (33, 45): ('рекомендации', 'рекомендация', работать, 'obl'), (56, 67): ('Организации', 'организация', рекомендации, 'nmod'), (68, 83): ('Здравоохранения', 'здравоохранение', Организации, 'nmod'), (93, 100): ('причине', 'причина', уволили, 'obl'), (101, 104): ('его', 'иванов', Иван, 'appos')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно в будущем не включать слова, входящие в чанк, если нам не интересны связи между частями чанка."
      ],
      "metadata": {
        "id": "KV5aXx_FVou1"
      }
    }
  ]
}